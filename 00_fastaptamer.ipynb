{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qgSaumvNBsHQ"
   },
   "source": [
    "### install sratoolkit\n",
    "1. download from NCBI: [SRA Toolkit](https://ftp-trace.ncbi.nlm.nih.gov/sra/sdk/current/sratoolkit.current-ubuntu64.tar.gz)\n",
    "\n",
    "2. unzip the file\n",
    "3. install sratoolkit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_SA7bTQpEDq5",
    "outputId": "af86192b-a270-4d54-f9cc-4bacdef90b99"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-03-03 09:06:54--  https://ftp-trace.ncbi.nlm.nih.gov/sra/sdk/current/sratoolkit.current-ubuntu64.tar.gz\n",
      "Resolving ftp-trace.ncbi.nlm.nih.gov (ftp-trace.ncbi.nlm.nih.gov)... 130.14.250.10, 130.14.250.11, 2607:f220:41e:250::11, ...\n",
      "Connecting to ftp-trace.ncbi.nlm.nih.gov (ftp-trace.ncbi.nlm.nih.gov)|130.14.250.10|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 93498661 (89M) [application/x-gzip]\n",
      "Saving to: ‘sratoolkit.tar.gz’\n",
      "\n",
      "sratoolkit.tar.gz   100%[===================>]  89.17M  41.7MB/s    in 2.1s    \n",
      "\n",
      "2024-03-03 09:06:57 (41.7 MB/s) - ‘sratoolkit.tar.gz’ saved [93498661/93498661]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget --output-document sratoolkit.tar.gz https://ftp-trace.ncbi.nlm.nih.gov/sra/sdk/current/sratoolkit.current-ubuntu64.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "o76hAyTVEY4b",
    "outputId": "d3b1a98a-f4e8-4933-a9a3-4d8515417c80"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sratoolkit.3.0.10-ubuntu64/\n",
      "sratoolkit.3.0.10-ubuntu64/CHANGES\n",
      "sratoolkit.3.0.10-ubuntu64/schema/\n",
      "sratoolkit.3.0.10-ubuntu64/schema/align/\n",
      "sratoolkit.3.0.10-ubuntu64/schema/align/align.vschema\n",
      "sratoolkit.3.0.10-ubuntu64/schema/align/seq.vschema\n",
      "sratoolkit.3.0.10-ubuntu64/schema/align/refseq.vschema\n",
      "sratoolkit.3.0.10-ubuntu64/schema/align/pileup-stats.vschema\n",
      "sratoolkit.3.0.10-ubuntu64/schema/align/qstat.vschema\n",
      "sratoolkit.3.0.10-ubuntu64/schema/align/mate-cache.vschema\n",
      "sratoolkit.3.0.10-ubuntu64/schema/ncbi/\n",
      "sratoolkit.3.0.10-ubuntu64/schema/ncbi/sra.vschema\n",
      "sratoolkit.3.0.10-ubuntu64/schema/ncbi/stats.vschema\n",
      "sratoolkit.3.0.10-ubuntu64/schema/ncbi/varloc.vschema\n",
      "sratoolkit.3.0.10-ubuntu64/schema/ncbi/spotname.vschema\n",
      "sratoolkit.3.0.10-ubuntu64/schema/ncbi/seq.vschema\n",
      "sratoolkit.3.0.10-ubuntu64/schema/ncbi/seq-graph.vschema\n",
      "sratoolkit.3.0.10-ubuntu64/schema/ncbi/wgs-contig.vschema\n",
      "sratoolkit.3.0.10-ubuntu64/schema/ncbi/trace.vschema\n",
      "sratoolkit.3.0.10-ubuntu64/schema/ncbi/clip.vschema\n",
      "sratoolkit.3.0.10-ubuntu64/schema/ncbi/ncbi.vschema\n",
      "sratoolkit.3.0.10-ubuntu64/schema/ncbi/pnbrdb.vschema\n",
      "sratoolkit.3.0.10-ubuntu64/schema/csra2/\n",
      "sratoolkit.3.0.10-ubuntu64/schema/csra2/stats.vschema\n",
      "sratoolkit.3.0.10-ubuntu64/schema/csra2/reference.vschema\n",
      "sratoolkit.3.0.10-ubuntu64/schema/csra2/read.vschema\n",
      "sratoolkit.3.0.10-ubuntu64/schema/csra2/csra2.vschema\n",
      "sratoolkit.3.0.10-ubuntu64/schema/sra/\n",
      "sratoolkit.3.0.10-ubuntu64/schema/sra/helicos.vschema\n",
      "sratoolkit.3.0.10-ubuntu64/schema/sra/ion-torrent.vschema\n",
      "sratoolkit.3.0.10-ubuntu64/schema/sra/pacbio.vschema\n",
      "sratoolkit.3.0.10-ubuntu64/schema/sra/abi.vschema\n",
      "sratoolkit.3.0.10-ubuntu64/schema/sra/illumina.vschema\n",
      "sratoolkit.3.0.10-ubuntu64/schema/sra/generic-fastq.vschema\n",
      "sratoolkit.3.0.10-ubuntu64/schema/sra/pevents.vschema\n",
      "sratoolkit.3.0.10-ubuntu64/schema/sra/454.vschema\n",
      "sratoolkit.3.0.10-ubuntu64/schema/sra/nanopore.vschema\n",
      "sratoolkit.3.0.10-ubuntu64/schema/vdb/\n",
      "sratoolkit.3.0.10-ubuntu64/schema/vdb/built-in.vschema\n",
      "sratoolkit.3.0.10-ubuntu64/schema/vdb/vdb.vschema\n",
      "sratoolkit.3.0.10-ubuntu64/schema/insdc/\n",
      "sratoolkit.3.0.10-ubuntu64/schema/insdc/sra.vschema\n",
      "sratoolkit.3.0.10-ubuntu64/schema/insdc/insdc.vschema\n",
      "sratoolkit.3.0.10-ubuntu64/schema/insdc/seq.vschema\n",
      "sratoolkit.3.0.10-ubuntu64/bin/\n",
      "sratoolkit.3.0.10-ubuntu64/bin/dump-ref-fasta\n",
      "sratoolkit.3.0.10-ubuntu64/bin/test-sra\n",
      "sratoolkit.3.0.10-ubuntu64/bin/abi-load.3\n",
      "sratoolkit.3.0.10-ubuntu64/bin/bam-load\n",
      "sratoolkit.3.0.10-ubuntu64/bin/abi-dump.3.0.10\n",
      "sratoolkit.3.0.10-ubuntu64/bin/abi-load.3.0.10\n",
      "sratoolkit.3.0.10-ubuntu64/bin/sra-pileup.3.0.10\n",
      "sratoolkit.3.0.10-ubuntu64/bin/vdb-lock.3\n",
      "sratoolkit.3.0.10-ubuntu64/bin/align-info.3.0.10\n",
      "sratoolkit.3.0.10-ubuntu64/bin/srapath.3.0.10\n",
      "sratoolkit.3.0.10-ubuntu64/bin/kar\n",
      "sratoolkit.3.0.10-ubuntu64/bin/fasterq-dump\n",
      "sratoolkit.3.0.10-ubuntu64/bin/fasterq-dump.3.0.10\n",
      "sratoolkit.3.0.10-ubuntu64/bin/kdbmeta.3\n",
      "sratoolkit.3.0.10-ubuntu64/bin/illumina-load\n",
      "sratoolkit.3.0.10-ubuntu64/bin/kar.3\n",
      "sratoolkit.3.0.10-ubuntu64/bin/pacbio-load\n",
      "sratoolkit.3.0.10-ubuntu64/bin/fastq-dump.3.0.10\n",
      "sratoolkit.3.0.10-ubuntu64/bin/kdbmeta\n",
      "sratoolkit.3.0.10-ubuntu64/bin/vdb-encrypt\n",
      "sratoolkit.3.0.10-ubuntu64/bin/sratools.3\n",
      "sratoolkit.3.0.10-ubuntu64/bin/vdb-validate.3.0.10\n",
      "sratoolkit.3.0.10-ubuntu64/bin/illumina-dump\n",
      "sratoolkit.3.0.10-ubuntu64/bin/prefetch.3\n",
      "sratoolkit.3.0.10-ubuntu64/bin/srapath\n",
      "sratoolkit.3.0.10-ubuntu64/bin/vdb-dump\n",
      "sratoolkit.3.0.10-ubuntu64/bin/fastq-dump.3\n",
      "sratoolkit.3.0.10-ubuntu64/bin/vdb-dump.3\n",
      "sratoolkit.3.0.10-ubuntu64/bin/bam-load.3\n",
      "sratoolkit.3.0.10-ubuntu64/bin/vdb-copy.3.0.10\n",
      "sratoolkit.3.0.10-ubuntu64/bin/helicos-load.3\n",
      "sratoolkit.3.0.10-ubuntu64/bin/illumina-dump.3.0.10\n",
      "sratoolkit.3.0.10-ubuntu64/bin/vdb-encrypt.3\n",
      "sratoolkit.3.0.10-ubuntu64/bin/sra-sort-cg.3.0.10\n",
      "sratoolkit.3.0.10-ubuntu64/bin/helicos-load.3.0.10\n",
      "sratoolkit.3.0.10-ubuntu64/bin/srf-load\n",
      "sratoolkit.3.0.10-ubuntu64/bin/vdb-copy\n",
      "sratoolkit.3.0.10-ubuntu64/bin/abi-dump\n",
      "sratoolkit.3.0.10-ubuntu64/bin/sff-load.3\n",
      "sratoolkit.3.0.10-ubuntu64/bin/sra-search\n",
      "sratoolkit.3.0.10-ubuntu64/bin/fastq-dump-orig.3.0.10\n",
      "sratoolkit.3.0.10-ubuntu64/bin/latf-load.3.0.10\n",
      "sratoolkit.3.0.10-ubuntu64/bin/vdb-validate.3\n",
      "sratoolkit.3.0.10-ubuntu64/bin/sra-pileup-orig.3.0.10\n",
      "sratoolkit.3.0.10-ubuntu64/bin/sra-stat.3\n",
      "sratoolkit.3.0.10-ubuntu64/bin/vdb-decrypt.3.0.10\n",
      "sratoolkit.3.0.10-ubuntu64/bin/vdb-decrypt\n",
      "sratoolkit.3.0.10-ubuntu64/bin/vdb-unlock.3\n",
      "sratoolkit.3.0.10-ubuntu64/bin/sra-search.3.0.10\n",
      "sratoolkit.3.0.10-ubuntu64/bin/align-info.3\n",
      "sratoolkit.3.0.10-ubuntu64/bin/sff-dump.3\n",
      "sratoolkit.3.0.10-ubuntu64/bin/vdb-validate\n",
      "sratoolkit.3.0.10-ubuntu64/bin/sam-dump.3\n",
      "sratoolkit.3.0.10-ubuntu64/bin/sff-dump.3.0.10\n",
      "sratoolkit.3.0.10-ubuntu64/bin/cg-load.3\n",
      "sratoolkit.3.0.10-ubuntu64/bin/sra-pileup.3\n",
      "sratoolkit.3.0.10-ubuntu64/bin/prefetch.3.0.10\n",
      "sratoolkit.3.0.10-ubuntu64/bin/srapath-orig.3.0.10\n",
      "sratoolkit.3.0.10-ubuntu64/bin/cache-mgr.3\n",
      "sratoolkit.3.0.10-ubuntu64/bin/ncbi/\n",
      "sratoolkit.3.0.10-ubuntu64/bin/ncbi/vdb-copy.kfg\n",
      "sratoolkit.3.0.10-ubuntu64/bin/ncbi/default.kfg\n",
      "sratoolkit.3.0.10-ubuntu64/bin/ncbi/certs.kfg\n",
      "sratoolkit.3.0.10-ubuntu64/bin/sra-stat\n",
      "sratoolkit.3.0.10-ubuntu64/bin/dump-ref-fasta.3.0.10\n",
      "sratoolkit.3.0.10-ubuntu64/bin/vdb-dump-orig.3.0.10\n",
      "sratoolkit.3.0.10-ubuntu64/bin/pacbio-load.3.0.10\n",
      "sratoolkit.3.0.10-ubuntu64/bin/rcexplain.3.0.10\n",
      "sratoolkit.3.0.10-ubuntu64/bin/kdbmeta.3.0.10\n",
      "sratoolkit.3.0.10-ubuntu64/bin/cache-mgr\n",
      "sratoolkit.3.0.10-ubuntu64/bin/sratools\n",
      "sratoolkit.3.0.10-ubuntu64/bin/sff-load\n",
      "sratoolkit.3.0.10-ubuntu64/bin/vdb-config.3\n",
      "sratoolkit.3.0.10-ubuntu64/bin/sam-dump-orig.3.0.10\n",
      "sratoolkit.3.0.10-ubuntu64/bin/pacbio-load.3\n",
      "sratoolkit.3.0.10-ubuntu64/bin/vdb-lock\n",
      "sratoolkit.3.0.10-ubuntu64/bin/srf-load.3\n",
      "sratoolkit.3.0.10-ubuntu64/bin/vdb-unlock.3.0.10\n",
      "sratoolkit.3.0.10-ubuntu64/bin/abi-load\n",
      "sratoolkit.3.0.10-ubuntu64/bin/sratools.3.0.10\n",
      "sratoolkit.3.0.10-ubuntu64/bin/illumina-dump.3\n",
      "sratoolkit.3.0.10-ubuntu64/bin/fasterq-dump.3\n",
      "sratoolkit.3.0.10-ubuntu64/bin/sra-sort.3.0.10\n",
      "sratoolkit.3.0.10-ubuntu64/bin/sam-dump.3.0.10\n",
      "sratoolkit.3.0.10-ubuntu64/bin/illumina-load.3\n",
      "sratoolkit.3.0.10-ubuntu64/bin/align-info\n",
      "sratoolkit.3.0.10-ubuntu64/bin/rcexplain\n",
      "sratoolkit.3.0.10-ubuntu64/bin/rcexplain.3\n",
      "sratoolkit.3.0.10-ubuntu64/bin/srf-load.3.0.10\n",
      "sratoolkit.3.0.10-ubuntu64/bin/fastq-load.3\n",
      "sratoolkit.3.0.10-ubuntu64/bin/prefetch-orig.3.0.10\n",
      "sratoolkit.3.0.10-ubuntu64/bin/fastq-dump\n",
      "sratoolkit.3.0.10-ubuntu64/bin/vdb-encrypt.3.0.10\n",
      "sratoolkit.3.0.10-ubuntu64/bin/sra-sort-cg.3\n",
      "sratoolkit.3.0.10-ubuntu64/bin/bam-load.3.0.10\n",
      "sratoolkit.3.0.10-ubuntu64/bin/sra-pileup\n",
      "sratoolkit.3.0.10-ubuntu64/bin/helicos-load\n",
      "sratoolkit.3.0.10-ubuntu64/bin/sra-search.3\n",
      "sratoolkit.3.0.10-ubuntu64/bin/sra-sort.3\n",
      "sratoolkit.3.0.10-ubuntu64/bin/fastq-load.3.0.10\n",
      "sratoolkit.3.0.10-ubuntu64/bin/vdb-unlock\n",
      "sratoolkit.3.0.10-ubuntu64/bin/latf-load\n",
      "sratoolkit.3.0.10-ubuntu64/bin/vdb-config.3.0.10\n",
      "sratoolkit.3.0.10-ubuntu64/bin/illumina-load.3.0.10\n",
      "sratoolkit.3.0.10-ubuntu64/bin/sra-sort\n",
      "sratoolkit.3.0.10-ubuntu64/bin/prefetch\n",
      "sratoolkit.3.0.10-ubuntu64/bin/fastq-load\n",
      "sratoolkit.3.0.10-ubuntu64/bin/kar.3.0.10\n",
      "sratoolkit.3.0.10-ubuntu64/bin/sra-sort-cg\n",
      "sratoolkit.3.0.10-ubuntu64/bin/vdb-dump.3.0.10\n",
      "sratoolkit.3.0.10-ubuntu64/bin/vdb-lock.3.0.10\n",
      "sratoolkit.3.0.10-ubuntu64/bin/test-sra.3\n",
      "sratoolkit.3.0.10-ubuntu64/bin/vdb-config\n",
      "sratoolkit.3.0.10-ubuntu64/bin/fasterq-dump-orig.3.0.10\n",
      "sratoolkit.3.0.10-ubuntu64/bin/cg-load.3.0.10\n",
      "sratoolkit.3.0.10-ubuntu64/bin/dump-ref-fasta.3\n",
      "sratoolkit.3.0.10-ubuntu64/bin/sff-load.3.0.10\n",
      "sratoolkit.3.0.10-ubuntu64/bin/cg-load\n",
      "sratoolkit.3.0.10-ubuntu64/bin/cache-mgr.3.0.10\n",
      "sratoolkit.3.0.10-ubuntu64/bin/latf-load.3\n",
      "sratoolkit.3.0.10-ubuntu64/bin/abi-dump.3\n",
      "sratoolkit.3.0.10-ubuntu64/bin/srapath.3\n",
      "sratoolkit.3.0.10-ubuntu64/bin/sam-dump\n",
      "sratoolkit.3.0.10-ubuntu64/bin/test-sra.3.0.10\n",
      "sratoolkit.3.0.10-ubuntu64/bin/vdb-decrypt.3\n",
      "sratoolkit.3.0.10-ubuntu64/bin/vdb-copy.3\n",
      "sratoolkit.3.0.10-ubuntu64/bin/sra-stat.3.0.10\n",
      "sratoolkit.3.0.10-ubuntu64/bin/sff-dump\n",
      "sratoolkit.3.0.10-ubuntu64/README-blastn\n",
      "sratoolkit.3.0.10-ubuntu64/README.md\n",
      "sratoolkit.3.0.10-ubuntu64/example/\n",
      "sratoolkit.3.0.10-ubuntu64/example/perl/\n",
      "sratoolkit.3.0.10-ubuntu64/example/perl/mismatch-stats.pl\n",
      "sratoolkit.3.0.10-ubuntu64/example/perl/dump-reference.pl\n",
      "sratoolkit.3.0.10-ubuntu64/example/perl/quality-stats.pl\n",
      "sratoolkit.3.0.10-ubuntu64/example/perl/simplefastq.pl\n",
      "sratoolkit.3.0.10-ubuntu64/example/perl/gene-lookup.pl\n",
      "sratoolkit.3.0.10-ubuntu64/example/perl/base-stats.pl\n",
      "sratoolkit.3.0.10-ubuntu64/example/perl/splitfastq.pl\n",
      "sratoolkit.3.0.10-ubuntu64/README-vdb-config\n"
     ]
    }
   ],
   "source": [
    "!tar -vxzf sratoolkit.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wesGXRqNEk-a"
   },
   "outputs": [],
   "source": [
    "!which fastq-dump"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NSW-XZarE_fr",
    "outputId": "a9dddd27-3a1b-4218-83c9-e62324da3346"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading package lists... Done\n",
      "Building dependency tree... Done\n",
      "Reading state information... Done\n",
      "The following additional packages will be installed:\n",
      "  blends-common libkdf5-2 libncbi-vdb2 libncbi-wvdb2 med-config menu\n",
      "Suggested packages:\n",
      "  blends-doc menu-l10n gksu | kde-runtime | ktsuss\n",
      "The following NEW packages will be installed:\n",
      "  blends-common libkdf5-2 libncbi-vdb2 libncbi-wvdb2 med-config menu\n",
      "  sra-toolkit\n",
      "0 upgraded, 7 newly installed, 0 to remove and 35 not upgraded.\n",
      "Need to get 8,290 kB of archives.\n",
      "After this operation, 23.0 MB of additional disk space will be used.\n",
      "Get:1 http://archive.ubuntu.com/ubuntu jammy/universe amd64 menu amd64 2.1.47ubuntu4 [354 kB]\n",
      "Get:2 http://archive.ubuntu.com/ubuntu jammy/universe amd64 blends-common all 0.7.4ubuntu1 [15.9 kB]\n",
      "Get:3 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libkdf5-2 amd64 2.11.2+dfsg-4build2 [14.7 kB]\n",
      "Get:4 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libncbi-vdb2 amd64 2.11.2+dfsg-4build2 [1,364 kB]\n",
      "Get:5 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libncbi-wvdb2 amd64 2.11.2+dfsg-4build2 [1,252 kB]\n",
      "Get:6 http://archive.ubuntu.com/ubuntu jammy/universe amd64 sra-toolkit amd64 2.11.3+dfsg-1ubuntu1 [5,277 kB]\n",
      "Get:7 http://archive.ubuntu.com/ubuntu jammy/universe amd64 med-config all 3.7.1 [11.6 kB]\n",
      "Fetched 8,290 kB in 2s (4,540 kB/s)\n",
      "debconf: unable to initialize frontend: Dialog\n",
      "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 78, <> line 7.)\n",
      "debconf: falling back to frontend: Readline\n",
      "debconf: unable to initialize frontend: Readline\n",
      "debconf: (This frontend requires a controlling tty.)\n",
      "debconf: falling back to frontend: Teletype\n",
      "dpkg-preconfigure: unable to re-open stdin: \n",
      "Selecting previously unselected package menu.\n",
      "(Reading database ... 121749 files and directories currently installed.)\n",
      "Preparing to unpack .../0-menu_2.1.47ubuntu4_amd64.deb ...\n",
      "Unpacking menu (2.1.47ubuntu4) ...\n",
      "Selecting previously unselected package blends-common.\n",
      "Preparing to unpack .../1-blends-common_0.7.4ubuntu1_all.deb ...\n",
      "Unpacking blends-common (0.7.4ubuntu1) ...\n",
      "Selecting previously unselected package libkdf5-2.\n",
      "Preparing to unpack .../2-libkdf5-2_2.11.2+dfsg-4build2_amd64.deb ...\n",
      "Unpacking libkdf5-2 (2.11.2+dfsg-4build2) ...\n",
      "Selecting previously unselected package libncbi-vdb2.\n",
      "Preparing to unpack .../3-libncbi-vdb2_2.11.2+dfsg-4build2_amd64.deb ...\n",
      "Unpacking libncbi-vdb2 (2.11.2+dfsg-4build2) ...\n",
      "Selecting previously unselected package libncbi-wvdb2.\n",
      "Preparing to unpack .../4-libncbi-wvdb2_2.11.2+dfsg-4build2_amd64.deb ...\n",
      "Unpacking libncbi-wvdb2 (2.11.2+dfsg-4build2) ...\n",
      "Selecting previously unselected package sra-toolkit.\n",
      "Preparing to unpack .../5-sra-toolkit_2.11.3+dfsg-1ubuntu1_amd64.deb ...\n",
      "Unpacking sra-toolkit (2.11.3+dfsg-1ubuntu1) ...\n",
      "Selecting previously unselected package med-config.\n",
      "Preparing to unpack .../6-med-config_3.7.1_all.deb ...\n",
      "Unpacking med-config (3.7.1) ...\n",
      "Setting up libncbi-vdb2 (2.11.2+dfsg-4build2) ...\n",
      "Setting up libkdf5-2 (2.11.2+dfsg-4build2) ...\n",
      "Setting up libncbi-wvdb2 (2.11.2+dfsg-4build2) ...\n",
      "Setting up menu (2.1.47ubuntu4) ...\n",
      "Setting up blends-common (0.7.4ubuntu1) ...\n",
      "Setting up sra-toolkit (2.11.3+dfsg-1ubuntu1) ...\n",
      "Setting up med-config (3.7.1) ...\n",
      "debconf: unable to initialize frontend: Dialog\n",
      "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 78.)\n",
      "debconf: falling back to frontend: Readline\n",
      "Adding group `med' (GID 107) ...\n",
      "Done.\n",
      "Processing triggers for man-db (2.10.2-1) ...\n",
      "Processing triggers for libc-bin (2.35-0ubuntu3.4) ...\n",
      "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n",
      "\n",
      "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n",
      "\n",
      "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n",
      "\n",
      "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n",
      "\n",
      "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n",
      "\n",
      "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n",
      "\n",
      "Processing triggers for menu (2.1.47ubuntu4) ...\n"
     ]
    }
   ],
   "source": [
    "!sudo apt install sra-toolkit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bBAYPHO_EviA"
   },
   "outputs": [],
   "source": [
    "# !fastq-dump --stdout -X 2 SRR12960220\n",
    "# !prefetch SRR12960220\n",
    "# !ls /content/SRR12960220\n",
    "# !fastq-dump --outdir fastq --gzip --skip-technical  --readids --read-filter pass --dumpbase --split-3 --clip /content/SRR12960220/SRR12960220.sra"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7MfkIas8cbb8"
   },
   "source": [
    "### download the SRR file\n",
    "1-prefetch\n",
    "\n",
    "2-unzip fastq files to fastq folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "K6yzjcFbn04s",
    "outputId": "b4e98353-598d-460b-ba00-9c8ccd80779d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Currently downloading: SRR12960220\n",
      "The command used was: prefetch SRR12960220\n",
      "Currently downloading: SRR12960218\n",
      "The command used was: prefetch SRR12960218\n",
      "Generating fastq for: SRR12960220\n",
      "The command used was: fastq-dump --outdir fastq --gzip --skip-technical  --readids --read-filter pass --dumpbase --split-3 --clip /content/SRR12960220/SRR12960220.sra\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "\n",
    "# samples correspond to round1_low_negative, round1_low_positive\n",
    "sra_numbers = [\n",
    "    \"SRR12960220\", \"SRR12960218\"\n",
    "    ]\n",
    "\n",
    "# this will download the .sra files to ~/ncbi/public/sra/ (will create directory if not present)\n",
    "for sra_id in sra_numbers:\n",
    "    print (\"Currently downloading: \" + sra_id)\n",
    "    prefetch = \"prefetch \" + sra_id\n",
    "    print (\"The command used was: \" + prefetch)\n",
    "    subprocess.call(prefetch, shell=True)\n",
    "\n",
    "# this will extract the .sra files from above into a folder named 'fastq'\n",
    "for sra_id in sra_numbers:\n",
    "    print (\"Generating fastq for: \" + sra_id)\n",
    "    fastq_dump = \"fastq-dump --outdir fastq --gzip --skip-technical  --readids --read-filter pass --dumpbase --split-3 --clip /content/\"+ sra_id +\"/\" + sra_id + \".sra\"\n",
    "    print (\"The command used was: \" + fastq_dump)\n",
    "    subprocess.call(fastq_dump, shell=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qgMIgDgjLBPK"
   },
   "outputs": [],
   "source": [
    "for sra_id in sra_numbers:\n",
    "  print(f\"Unzip fastq file of {sra_id} in fastq folder\")\n",
    "  fastq_unzip = \"gzip -dk /content/fastq/\" + sra_id + \"_pass_1.fastq.gz\"\n",
    "  subprocess.call(fastq_unzip, shell=True)\n",
    "\n",
    "# !gzip -dk /content/fastq/SRR12960220_pass_1.fastq.gz\n",
    "# /content/fastq/SRR12960220_pass_1.fastq.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yAYQvHxzmLI2"
   },
   "source": [
    "### Changing fastq to fasta\n",
    "\n",
    "1- granting exeucatable to fastaptamer_count file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2hW9o9EIm-BM"
   },
   "outputs": [],
   "source": [
    "#fastaptamer_count_dir = \"/content/drive/MyDrive/BioProj/FastaProj/fastaptamer_count\"\n",
    "\n",
    "!chmod +x /content/drive/MyDrive/BioProj/FastaProj/fastaptamer_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Lo0k5_FeL2Kk",
    "outputId": "3a7819dd-6543-46a3-e1a7-f2cca5b7b631"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "153486985 total sequences.  25240875 unique sequences.\n",
      "Input file: \"/content/fastq/SRR12960220_pass_1.fastq\".\n",
      "Output file: \"/content/fastq/SRR12960220_pass_1.fasta\".\n",
      "Execution time: 670 s.\n"
     ]
    }
   ],
   "source": [
    "for sra_id in sra_numbers:\n",
    "  print(f\"changing fastq file of {sra_id} to fasta file\")\n",
    "  fastq_to_fasta = f\"/content/drive/MyDrive/BioProj/FastaProj/fastaptamer_count \" \\\n",
    "                 f\"-i/content/fastq/{sra_id}_pass_1.fastq -o /content/fastq/{sra_id}.fasta\"\n",
    "  subprocess.call(fastq_to_fasta, shell=True)\n",
    "\n",
    "# ! /content/drive/MyDrive/BioProj/FastaProj/fastaptamer_count -i /content/fastq/SRR12960220_pass_1.fastq -o /content/fastq/SRR12960220_pass_1.fasta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3hgniOF8qeMr"
   },
   "source": [
    "### install Biopython\n",
    "\n",
    "import pandas, make the list by fasta files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kXTW5Q7HyByb",
    "outputId": "bf77b2a9-12d1-4dee-ef95-56436accf131"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting Biopython\n",
      "  Downloading biopython-1.83-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from Biopython) (1.23.5)\n",
      "Installing collected packages: Biopython\n",
      "Successfully installed Biopython-1.83\n"
     ]
    }
   ],
   "source": [
    "!pip install Biopython\n",
    "from Bio import SeqIO\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NziMGYkRx6Hi"
   },
   "outputs": [],
   "source": [
    "for sra_id in sra_numbers:\n",
    "  if sra_id == \"SRR12960218\":\n",
    "    seq_objects_positive = SeqIO.parse(f\"/content/fastq/{sra_id}.fasta\",'fasta')\n",
    "    seq_round1_low_positive=[]\n",
    "    ids_round1_low_positive=[]\n",
    "    for record in seq_objects_positive:\n",
    "      seq_round1_low_positive.append(str(record.seq))\n",
    "      ids_round1_low_positive.append(record.id)\n",
    "  else:\n",
    "    seq_objects_negative = SeqIO.parse(f\"/content/fastq/{sra_id}.fasta\",'fasta')\n",
    "    seq_round1_low_negative=[]\n",
    "    ids_round1_low_negative=[]\n",
    "    for record in seq_objects_negative:\n",
    "      seq_round1_low_negative.append(str(record.seq))\n",
    "      ids_round1_low_negative.append(record.id)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hc9ot6YlgeTM"
   },
   "source": [
    "\n",
    "## here i wrote the code, cell after i should revise and update\n",
    "\n",
    "https://medium.com/tensorflow/using-nucleus-and-tensorflow-for-dna-sequencing-error-correction-47f3f7fc1a50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RXnkKEA2xt92",
    "outputId": "573a9fbb-8384-4699-bd44-6b1ad35edf99"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25240875"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lL8ZIv1kzu-B"
   },
   "outputs": [],
   "source": [
    "ranks = []\n",
    "reads = []\n",
    "rpms = []\n",
    "\n",
    "for item in ids:\n",
    "  values = item.split(\"-\")\n",
    "\n",
    "  rank = int(values[0])\n",
    "  read = int(values[1])\n",
    "  rpm = float(values[2])\n",
    "\n",
    "  ranks.append(rank)\n",
    "  reads.append(read)\n",
    "  rpms.append(rpm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "adfXjfKZzlqq"
   },
   "outputs": [],
   "source": [
    "count_1 = pd.DataFrame({'Sequence': sequences, 'ID': ids})\n",
    "count_2 = pd.DataFrame({'Rank': ranks, 'Read': reads, 'RPM':rpms, 'Sequence': sequences})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "nvscjvJb0d7K",
    "outputId": "2183bb98-71b0-499e-e9e6-3fe82919be6a"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <div id=\"df-3c343f0d-4aad-475d-ad66-c7913b686e69\" class=\"colab-df-container\">\n",
       "    <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Rank</th>\n",
       "      <th>Read</th>\n",
       "      <th>RPM</th>\n",
       "      <th>Sequence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>14343</td>\n",
       "      <td>93.45</td>\n",
       "      <td>GGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>8480</td>\n",
       "      <td>55.25</td>\n",
       "      <td>CGTGCTACCGTGAAGAATTCCGCCCAGATCATCTCGTATG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>4005</td>\n",
       "      <td>26.09</td>\n",
       "      <td>AGAATAGTCACAATAAGTTTAACCAAAAGGTTTTAATCCAC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>3900</td>\n",
       "      <td>25.41</td>\n",
       "      <td>CTTAAAGTGATTCATCGTCTTTTATCGTCATTTCAATATTT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>3698</td>\n",
       "      <td>24.09</td>\n",
       "      <td>GATTTGATAGTTTTCAGTAGTTTAAAGACTCATCAGGAAGC</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "    <div class=\"colab-df-buttons\">\n",
       "\n",
       "  <div class=\"colab-df-container\">\n",
       "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-3c343f0d-4aad-475d-ad66-c7913b686e69')\"\n",
       "            title=\"Convert this dataframe to an interactive table.\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
       "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
       "  </svg>\n",
       "    </button>\n",
       "\n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    .colab-df-buttons div {\n",
       "      margin-bottom: 4px;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "    <script>\n",
       "      const buttonEl =\n",
       "        document.querySelector('#df-3c343f0d-4aad-475d-ad66-c7913b686e69 button.colab-df-convert');\n",
       "      buttonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "      async function convertToInteractive(key) {\n",
       "        const element = document.querySelector('#df-3c343f0d-4aad-475d-ad66-c7913b686e69');\n",
       "        const dataTable =\n",
       "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                    [key], {});\n",
       "        if (!dataTable) return;\n",
       "\n",
       "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "          + ' to learn more about interactive tables.';\n",
       "        element.innerHTML = '';\n",
       "        dataTable['output_type'] = 'display_data';\n",
       "        await google.colab.output.renderOutput(dataTable, element);\n",
       "        const docLink = document.createElement('div');\n",
       "        docLink.innerHTML = docLinkHtml;\n",
       "        element.appendChild(docLink);\n",
       "      }\n",
       "    </script>\n",
       "  </div>\n",
       "\n",
       "\n",
       "<div id=\"df-8af9fed0-b829-4217-97bd-348564da6913\">\n",
       "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-8af9fed0-b829-4217-97bd-348564da6913')\"\n",
       "            title=\"Suggest charts\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "     width=\"24px\">\n",
       "    <g>\n",
       "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
       "    </g>\n",
       "</svg>\n",
       "  </button>\n",
       "\n",
       "<style>\n",
       "  .colab-df-quickchart {\n",
       "      --bg-color: #E8F0FE;\n",
       "      --fill-color: #1967D2;\n",
       "      --hover-bg-color: #E2EBFA;\n",
       "      --hover-fill-color: #174EA6;\n",
       "      --disabled-fill-color: #AAA;\n",
       "      --disabled-bg-color: #DDD;\n",
       "  }\n",
       "\n",
       "  [theme=dark] .colab-df-quickchart {\n",
       "      --bg-color: #3B4455;\n",
       "      --fill-color: #D2E3FC;\n",
       "      --hover-bg-color: #434B5C;\n",
       "      --hover-fill-color: #FFFFFF;\n",
       "      --disabled-bg-color: #3B4455;\n",
       "      --disabled-fill-color: #666;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart {\n",
       "    background-color: var(--bg-color);\n",
       "    border: none;\n",
       "    border-radius: 50%;\n",
       "    cursor: pointer;\n",
       "    display: none;\n",
       "    fill: var(--fill-color);\n",
       "    height: 32px;\n",
       "    padding: 0;\n",
       "    width: 32px;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart:hover {\n",
       "    background-color: var(--hover-bg-color);\n",
       "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "    fill: var(--button-hover-fill-color);\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart-complete:disabled,\n",
       "  .colab-df-quickchart-complete:disabled:hover {\n",
       "    background-color: var(--disabled-bg-color);\n",
       "    fill: var(--disabled-fill-color);\n",
       "    box-shadow: none;\n",
       "  }\n",
       "\n",
       "  .colab-df-spinner {\n",
       "    border: 2px solid var(--fill-color);\n",
       "    border-color: transparent;\n",
       "    border-bottom-color: var(--fill-color);\n",
       "    animation:\n",
       "      spin 1s steps(1) infinite;\n",
       "  }\n",
       "\n",
       "  @keyframes spin {\n",
       "    0% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "      border-left-color: var(--fill-color);\n",
       "    }\n",
       "    20% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    30% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    40% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    60% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    80% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "    90% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "  }\n",
       "</style>\n",
       "\n",
       "  <script>\n",
       "    async function quickchart(key) {\n",
       "      const quickchartButtonEl =\n",
       "        document.querySelector('#' + key + ' button');\n",
       "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
       "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
       "      try {\n",
       "        const charts = await google.colab.kernel.invokeFunction(\n",
       "            'suggestCharts', [key], {});\n",
       "      } catch (error) {\n",
       "        console.error('Error during call to suggestCharts:', error);\n",
       "      }\n",
       "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
       "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
       "    }\n",
       "    (() => {\n",
       "      let quickchartButtonEl =\n",
       "        document.querySelector('#df-8af9fed0-b829-4217-97bd-348564da6913 button');\n",
       "      quickchartButtonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "    })();\n",
       "  </script>\n",
       "</div>\n",
       "    </div>\n",
       "  </div>\n"
      ],
      "text/plain": [
       "   Rank   Read    RPM                                   Sequence\n",
       "0     1  14343  93.45   GGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGG\n",
       "1     2   8480  55.25   CGTGCTACCGTGAAGAATTCCGCCCAGATCATCTCGTATG\n",
       "2     3   4005  26.09  AGAATAGTCACAATAAGTTTAACCAAAAGGTTTTAATCCAC\n",
       "3     4   3900  25.41  CTTAAAGTGATTCATCGTCTTTTATCGTCATTTCAATATTT\n",
       "4     5   3698  24.09  GATTTGATAGTTTTCAGTAGTTTAAAGACTCATCAGGAAGC"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RnqzQhZW0pGZ"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OXKVl3iF0ddz"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NxOou8J4HJ1E"
   },
   "outputs": [],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import ticker\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import os\n",
    "\n",
    "import imblearn\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bmu7XVuBHXdx"
   },
   "outputs": [],
   "source": [
    "# reading file\n",
    "\n",
    "import zipfile\n",
    "\n",
    "!wget https://storage.googleapis.com/ztm_tf_course/food_vision/10_food_classes_10_percent.zip\n",
    "\n",
    "zip_ref = zipfile.ZipFile(\"10_food_classes_10_percent.zip\")\n",
    "zip_ref.extractall()\n",
    "zip_ref.close()\n",
    "\n",
    "# or\n",
    "unzip_data(\"xxx.zip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dCgbGGYr_Bfj"
   },
   "outputs": [],
   "source": [
    "# One_hot encoding\n",
    "# https://www.kaggle.com/code/zakarii/dna-sequence-classification-cnn-gru?scriptVersionId=60362406&cellId=13\n",
    "\n",
    "def encode_seq(s):\n",
    "    Encode = {'A':[1,0,0,0],'T':[0,1,0,0],'C':[0,0,1,0],'G':[0,0,0,1]}\n",
    "    return [Encode[x] for x in s]\n",
    "\n",
    "for i in sequence:\n",
    "    x = encode_seq(i)\n",
    "    encoded_list.append(x)\n",
    "\n",
    "X = np.array(encoded_list)\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "n85_JrZDkZPT"
   },
   "outputs": [],
   "source": [
    "class FeedForwardTrainer:\n",
    "  \"\"\"Class for training a simple feedforward net on aptamers data.\n",
    "\n",
    "  This class constructs the TF graph we need to train a model on aptamer data\n",
    "  and to evaluate the model on the validation data during training. It also\n",
    "  provides a method to actually run the training and uses some helper methods\n",
    "  to handle reporting training progress.\n",
    "\n",
    "  During training, this class also measures validation error. Therefore it also\n",
    "  builds the graph needed for making predictions on new data using feed_dict.\n",
    "  To evaluate the model, we feed serialized example protos to self.val_strs.\n",
    "\n",
    "  Attributes:\n",
    "    hps: The configuration object holding values of training metaparameters.\n",
    "      The trainer object just expects hps to have a learn_rate and momentum\n",
    "      attribute, but if the network uses the same object more attributes are\n",
    "      needed. Usually we expect hps to be an instance of tf.HParams and in\n",
    "      order for restoring the model to work it will need to be.\n",
    "    experiment_proto: selection_pb2.Experiment describing the experiment.\n",
    "    net: The neural net we are training.\n",
    "    train_dir: The folder we write checkpoints and other data to.\n",
    "    global_step: TF variable holding the global step counter\n",
    "  \"\"\"\n",
    "\n",
    "def __init__(self, hps, net, output_layer, experiment_proto, input_paths):\n",
    "    inputs, outputs = data.input_pipeline(\n",
    "        input_paths, experiment_proto, hps.mbsz, hps=hps, num_threads=8)\n",
    "    with tf.name_scope('neural_net'):\n",
    "      logits = net.fprop(inputs, mode='train')\n",
    "    with tf.name_scope('output_layer'):\n",
    "      loss_per_target = output_layer.average_loss_per_target(\n",
    "          logits, outputs, include_array=hps.train_on_array)\n",
    "      loss = utils.reduce_nanmean(loss_per_target)\n",
    "\n",
    "    self.global_step = tf.Variable(0, name='global_step', trainable=False)\n",
    "    if hps.optimizer == 'momentum':\n",
    "      optimizer = tf.MomentumOptimizer(hps.learn_rate, hps.momentum)\n",
    "    elif hps.optimizer == 'adam':\n",
    "      optimizer = tf.AdamOptimizer(hps.learn_rate)\n",
    "    else:\n",
    "      raise ValueError('invalid optimizer: %s' % hps.optimizer)\n",
    "    optimizer = tf.MomentumOptimizer(hps.learn_rate, hps.momentum)\n",
    "    grads = optimizer.compute_gradients(loss, net.params + output_layer.params)\n",
    "    opt_op = optimizer.apply_gradients(grads, global_step=self.global_step)\n",
    "    self.train_op = tf.with_dependencies([opt_op], loss)\n",
    "\n",
    "    contrib_deprecated.scalar_summary('loss/mean', loss)\n",
    "    for target in loss_per_target.axes['target'].labels:\n",
    "      contrib_deprecated.scalar_summary(\n",
    "          'loss/' + six.ensure_str(target),\n",
    "          lt.select(loss_per_target, {'target': target}))\n",
    "    with tf.name_scope('summarize_grads'):\n",
    "      slim.learning.add_gradients_summaries(grads)\n",
    "\n",
    "    tf.add_to_collection(tf.GraphKeys.GLOBAL_STEP, self.global_step)\n",
    "    tf.add_to_collection('train_op', self.train_op)\n",
    "    tf.add_to_collection('loss', loss)\n",
    "\n",
    "    self.mbsz = hps.mbsz\n",
    "    # The log Poisson loss implemented in TensorFlow may sometimes be negative.\n",
    "    if (hps.loss_name == output_layers.LOSS_POISSON_LOSS or\n",
    "        hps.loss_name == output_layers.LOSS_ZERO_TRUNCATED_POISSON_LOSS):\n",
    "      self.min_cost = -float('inf')\n",
    "      self.min_is_inclusive = False\n",
    "    else:\n",
    "      self.min_cost = 0\n",
    "      self.min_is_inclusive = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w75grgSCkN6s"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8zAxcUffcZ5Y"
   },
   "outputs": [],
   "source": [
    "def run_training(hps,\n",
    "                 experiment_proto,\n",
    "                 train_dir,\n",
    "                 train_input_paths,\n",
    "                 val_input_paths,\n",
    "                 tuner=None,\n",
    "                 master='',\n",
    "                 metrics_targets=None,\n",
    "                 metrics_measures=None):\n",
    "  \"\"\"Main training function.\n",
    "\n",
    "  Trains the model given a directory to write to and a logfile to write to.\n",
    "\n",
    "  Args:\n",
    "    hps: tf.HParams with training parameters.\n",
    "    experiment_proto: selection_pb2.Experiment proto for training.\n",
    "    train_dir: str path to train directory.\n",
    "    train_input_paths: List[str] giving paths to input sstables for training.\n",
    "    val_input_paths: List[str] giving paths to input sstable(s) for validation.\n",
    "    tuner: optional hp_tuner.HPTuner.\n",
    "    master: optional string to pass to a tf.Supervisor.\n",
    "    metrics_targets: String list of network targets to report metrics for.\n",
    "    metrics_measures: Measurements about the performance of the network to\n",
    "        report, e.g. 'auc/top_1p'.\n",
    "\n",
    "  Returns:\n",
    "    None.\n",
    "\n",
    "  Raises:\n",
    "    Error: if the hyperparamter combination in hps is infeasible and there is\n",
    "    no tuner. (If the hyperparameter combination is infeasible and there is\n",
    "    a tuner then the params are reported back to the tuner as infeasible.)\n",
    "  \"\"\"\n",
    "  hps_infeasible, infeasible_reason = hps_is_infeasible(\n",
    "      hps, experiment_proto.sequence_length)\n",
    "  if hps_infeasible:\n",
    "    if tuner:\n",
    "      tuner.report_done(True, infeasible_reason)\n",
    "      logger.info('report_done(infeasible=%r)', hps_infeasible)\n",
    "      return\n",
    "    else:\n",
    "      raise Error('Hyperparams are infeasible: %s', infeasible_reason)\n",
    "\n",
    "  logger.info('Starting training.')\n",
    "  if tuner:\n",
    "    logger.info('Using tuner: loaded HParams from Vizier')\n",
    "  else:\n",
    "    logger.info('No tuner: using default HParams')\n",
    "  logger.info('experiment_proto: %s', experiment_proto)\n",
    "  logger.info('train_dir: %s', train_dir)\n",
    "  logger.info('train_input_paths[0]: %s', train_input_paths[0])\n",
    "  logger.info('val_input_paths[0]: %s', val_input_paths[0])\n",
    "  logger.info('%r', list(hps.values()))\n",
    "  generationinfo.to_file(os.path.join(train_dir, 'geninfo.pbtxt'))\n",
    "  with gfile.Open(os.path.join(train_dir, config.hparams_name), 'w') as f:\n",
    "    f.write(str(hps.to_proto()))\n",
    "\n",
    "  eval_size = hps.eval_size or None\n",
    "\n",
    "  def make_subdir(subdirectory_mame):\n",
    "    path = os.path.join(train_dir, subdirectory_mame)\n",
    "    gfile.MakeDirs(path)\n",
    "    return path\n",
    "\n",
    "  logger.info('Computing preprocessing statistics')\n",
    "  # TODO(shoyer): move this over into preprocessing instead?\n",
    "  experiment_proto = dataset_stats.compute_experiment_statistics(\n",
    "      experiment_proto,\n",
    "      train_input_paths,\n",
    "      os.path.join(\n",
    "          hps.input_dir,\n",
    "          six.ensure_str(\n",
    "              config.wetlab_experiment_train_pbtxt_path[hps.val_fold]) +\n",
    "          '.wstats'),\n",
    "      preprocess_mode=hps.preprocess_mode,\n",
    "      max_size=eval_size,\n",
    "      logdir=make_subdir('compute-statistics'),\n",
    "      save_stats=hps.save_stats)\n",
    "\n",
    "  logging.info('Saving experiment proto with statistics')\n",
    "  with gfile.Open(\n",
    "      os.path.join(train_dir, config.wetlab_experiment_train_name), 'w') as f:\n",
    "    f.write(str(experiment_proto))\n",
    "\n",
    "  logger.debug(str(hps.to_proto()))\n",
    "  logger.debug(hps.run_name)\n",
    "\n",
    "  tr_entries = len(sstable.MergedSSTable(train_input_paths))\n",
    "  logger.info('Training sstable size: %d', tr_entries)\n",
    "  val_entries = len(sstable.MergedSSTable(val_input_paths))\n",
    "  logger.info('Validation sstable size: %d', val_entries)\n",
    "\n",
    "  epoch_size = hps.epoch_size or int(tr_entries * (1 + hps.ratio_random_dna))\n",
    "  num_batches_per_epoch = int(float(epoch_size) / hps.mbsz)\n",
    "\n",
    "  eval_ff.config_pandas_display(FLAGS.interactive_display)\n",
    "  tr_evaluator = eval_ff.Evaluator(\n",
    "      hps,\n",
    "      experiment_proto,\n",
    "      train_input_paths,\n",
    "      make_subdir(config.experiment_training_dir),\n",
    "      verbose=FLAGS.verbose_eval)\n",
    "  val_evaluator = eval_ff.Evaluator(\n",
    "      hps,\n",
    "      experiment_proto,\n",
    "      val_input_paths,\n",
    "      make_subdir(config.experiment_validation_dir),\n",
    "      verbose=FLAGS.verbose_eval)\n",
    "\n",
    "  with tf.Graph().as_default():\n",
    "    # we need to use the registered key 'hparams'\n",
    "    tf.add_to_collection('hparams', hps)\n",
    "\n",
    "    # TODO(shoyer): collect these into a Model class:\n",
    "    dummy_inputs = data.dummy_inputs(\n",
    "        experiment_proto,\n",
    "        input_features=hps.input_features,\n",
    "        kmer_k_max=hps.kmer_k_max,\n",
    "        additional_output=six.ensure_str(hps.additional_output).split(','))\n",
    "    output_layer = output_layers.create_output_layer(experiment_proto, hps)\n",
    "    net = ff.FeedForward(dummy_inputs, output_layer.logit_axis, hps)\n",
    "\n",
    "    trainer = FeedForwardTrainer(hps, net, output_layer, experiment_proto,\n",
    "                                 train_input_paths)\n",
    "\n",
    "    summary_writer = tf.SummaryWriter(make_subdir('training'), flush_secs=30)\n",
    "\n",
    "    # TODO(shoyer): file a bug to figure out why write_version=2 (now the\n",
    "    # default) doesn't work.\n",
    "    saver = tf.Saver(write_version=1)\n",
    "\n",
    "    # We are always the chief since we do not do distributed training.\n",
    "    # Every replica with a different task id is completely independent and all\n",
    "    # must be their own chief.\n",
    "    sv = tf.Supervisor(\n",
    "        logdir=train_dir,\n",
    "        is_chief=True,\n",
    "        summary_writer=summary_writer,\n",
    "        save_summaries_secs=10,\n",
    "        save_model_secs=180,\n",
    "        saver=saver)\n",
    "\n",
    "    logger.info('Preparing session')\n",
    "\n",
    "    train_report_dir = os.path.join(train_dir, config.experiment_training_dir)\n",
    "    cur_train_report = os.path.join(train_report_dir,\n",
    "                                    config.experiment_report_name)\n",
    "    best_train_report = os.path.join(train_report_dir,\n",
    "                                     config.experiment_best_report_name)\n",
    "\n",
    "    valid_report_dir = os.path.join(train_dir, config.experiment_validation_dir)\n",
    "    cur_valid_report = os.path.join(valid_report_dir,\n",
    "                                    config.experiment_report_name)\n",
    "    best_valid_report = os.path.join(valid_report_dir,\n",
    "                                     config.experiment_best_report_name)\n",
    "\n",
    "    best_checkpoint = os.path.join(train_dir, 'model.ckpt-lowest_val_loss')\n",
    "    best_checkpoint_meta = best_checkpoint + '.meta'\n",
    "    best_epoch_file = os.path.join(train_dir, 'best_epoch.txt')\n",
    "\n",
    "    with sv.managed_session(master) as sess:\n",
    "\n",
    "      logger.info('Starting queue runners')\n",
    "      sv.start_queue_runners(sess)\n",
    "\n",
    "      def save_and_evaluate():\n",
    "        \"\"\"Save and evaluate the current model.\n",
    "\n",
    "        Returns:\n",
    "          path: the path string to the checkpoint.\n",
    "          summary_df: pandas.DataFrame storing the evaluation result on the\n",
    "            validation dataset with rows for each output name and columns for\n",
    "            each metric value\n",
    "        \"\"\"\n",
    "        logger.info('Saving model checkpoint')\n",
    "        path = sv.saver.save(\n",
    "            sess,\n",
    "            sv.save_path,\n",
    "            global_step=sv.global_step,\n",
    "            write_meta_graph=True)\n",
    "        tr_evaluator.run(path, eval_size)\n",
    "        summary_df, _ = val_evaluator.run_and_report(\n",
    "            tuner,\n",
    "            path,\n",
    "            eval_size,\n",
    "            metrics_targets=metrics_targets,\n",
    "            metrics_measures=metrics_measures)\n",
    "        return path, summary_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hXckQ-mrjDEv"
   },
   "outputs": [],
   "source": [
    "def run_training_with_default_inputs(hps,\n",
    "                                     train_dir,\n",
    "                                     tuner=None,\n",
    "                                     master='',\n",
    "                                     val_fold_template='',\n",
    "                                     metrics_targets=None,\n",
    "                                     metrics_measures=None):\n",
    "  \"\"\"Start a training run with default inputs.\n",
    "\n",
    "  Args:\n",
    "    hps: tf.HParams with training parameters.\n",
    "    train_dir: str path to train directory.\n",
    "    tuner: optional hp_tuner.HPTuner.\n",
    "    master: optional string to pass to a tf.Supervisor.\n",
    "    val_fold_template: optional string to use to change the name of the\n",
    "      validation fold data from its default.\n",
    "    metrics_targets: String list of network targets to report metrics for.\n",
    "    metrics_measures: Measurements about the performance of the network to\n",
    "        report, e.g. 'auc/top_1p'.\n",
    "\n",
    "  Raises:\n",
    "    ValueError: Proto inconsistent with input feature.\n",
    "\n",
    "  Returns:\n",
    "    None.\n",
    "  \"\"\"\n",
    "  gfile.MakeDirs(train_dir)\n",
    "\n",
    "  task_log_path = os.path.join(train_dir, 'train_feedforward.log')\n",
    "  io_utils.log_to_stderr_and_file(\n",
    "      task_log_path, loggers=[logger, eval_ff.logger])\n",
    "\n",
    "  logger.info('Setting up fold')\n",
    "  experiment_proto, train_input_paths, val_input_paths = _setup_fold(\n",
    "      hps.input_dir, train_dir, hps.val_fold, val_fold_template)\n",
    "\n",
    "  if hps.additional_output:\n",
    "    existing_ao = [ao.name for ao in experiment_proto.additional_output]\n",
    "    for idx, name in enumerate(\n",
    "        six.ensure_str(hps.additional_output).split(',')):\n",
    "      if name not in existing_ao and name:\n",
    "        experiment_proto.additional_output.add(\n",
    "            name=name,\n",
    "            measurement_id=idx)\n",
    "  if data.STRUCTURE_PARTITION_FUNCTION in hps.input_features:\n",
    "    if not experiment_proto.has_partition_function:\n",
    "      raise ValueError(\n",
    "          'invalid input_feature for proto lacking partition function: %s' %\n",
    "          (data.STRUCTURE_PARTITION_FUNCTION))\n",
    "\n",
    "  try:\n",
    "    run_training(\n",
    "        hps,\n",
    "        experiment_proto,\n",
    "        train_dir,\n",
    "        train_input_paths,\n",
    "        val_input_paths,\n",
    "        tuner=tuner,\n",
    "        master=master,\n",
    "        metrics_targets=metrics_targets,\n",
    "        metrics_measures=metrics_measures)\n",
    "  except Exception:  # pylint: disable=broad-except\n",
    "    # ensure errors end up in the logs\n",
    "    logger.exception('Error encountered in run_training:')\n",
    "    # re-raise to fail the task\n",
    "    raise\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
